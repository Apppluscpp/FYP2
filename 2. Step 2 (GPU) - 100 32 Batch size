import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import sqlite3
import json
import torch
from tqdm import tqdm
from huggingface_hub import hf_hub_download
from llama_prompt import InstructionPrompt
from llama_structure import Llama3Model
from llama_training_speedup_utils import Tokenizer, generate_text_simple
from parameters import LLAMA32_CONFIG_1B
from config import finetuned_GQA_model_path 

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === Step 1: Download DB from Hugging Face ===

def fetch_db_from_huggingface(repo_id, filename, token):
    db_path = hf_hub_download(
        repo_id=repo_id,
        filename=filename,
        repo_type="dataset",
        token=token
    )
    print(f"[üì•] Downloaded database to: {db_path}")
    return db_path


# === Step 2: Extract Queries from All Tables ===

def extract_questions_from_all_tables(db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = [row[0] for row in cursor.fetchall()]

    questions = []
    for table in tables:
        try:
            cursor.execute(f"PRAGMA table_info({table});")
            columns = [row[1] for row in cursor.fetchall()]
            if 'question' in columns:
                cursor.execute(f"SELECT question FROM {table};")
                rows = cursor.fetchall()
                questions.extend([row[0] for row in rows])
            else:
                print(f"[‚ö†Ô∏è] Table {table} has no 'question' column.")
        except Exception as e:
            print(f"[‚ùå] Error in table {table}: {e}")

    conn.close()
    return questions


# === Step 3: Format Prompts ===

def generate_instruction_prompts(queries, system_prompt="You are a helpful assistant"):
    formatter = InstructionPrompt(system_prompt=system_prompt)
    prompts = [formatter.generate_inference_prompt(q) for q in queries]
    return prompts


# === Step 4: Save Prompts to File ===

def save_prompts_to_file(prompts, filename="rejection_sampling_prompts.txt"):
    with open(filename, "w", encoding="utf-8") as f:
        for prompt in prompts:
            f.write(prompt + "\n\n")
    print(f"[‚úÖ] Saved {len(prompts)} prompts to {filename}")


# === Step 5: Sample Completions ===

def sample_completions(model, tokenizer, prompt_file, output_file, max_prompts=10000, completions_per_prompt=4, batch_size=32):
    with open(prompt_file, "r", encoding="utf-8") as f:
        all_prompts = f.read().strip().split("\n\n")

    prompts = all_prompts[:max_prompts]
    all_samples = []

    print(f"[üéØ] Sampling {completions_per_prompt} completions each for {len(prompts)} prompts (batch size = {batch_size})...")

    for i in tqdm(range(0, len(prompts), batch_size), desc="Sampling"):
        batch_prompts = prompts[i:i + batch_size]
        batch_encoded = []

        for prompt in batch_prompts:
            ids = tokenizer.encode(prompt)
            batch_encoded.append(torch.tensor(ids, dtype=torch.long))

        # Pad to max length
        max_len = max([len(x) for x in batch_encoded])
        batch_tensor = torch.stack([
            torch.cat([x, torch.zeros(max_len - len(x), dtype=torch.long)])
            for x in batch_encoded
        ])

        batch_tensor = batch_tensor.to(device)  # (B, T)

        batch_completions = []
        for _ in range(completions_per_prompt):
            output_ids = generate_text_simple(
                model=model,
                idx=batch_tensor.clone(),  # (B, T)
                max_new_tokens=256,
                context_size=LLAMA32_CONFIG_1B["context_length"]
            )

            for j in range(len(batch_prompts)):
                generated_tokens = output_ids[j][len(batch_encoded[j]):]
                decoded = tokenizer.decode(generated_tokens.tolist()).strip()
                if len(batch_completions) < len(batch_prompts):
                    batch_completions.append([decoded])
                else:
                    batch_completions[j].append(decoded)

        for prompt, completions in zip(batch_prompts, batch_completions):
            all_samples.append({
                "prompt": prompt,
                "completions": completions
            })

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_samples, f, indent=2)
    print(f"[‚úÖ] Saved completions to {output_file}")
    


# === MAIN ===

if __name__ == "__main__":
    print("[üîÅ] Starting Rejection Sampling Prompt Generation Pipeline...")

    # Step 1: Download the instruction fine-tune DB
    db_path = fetch_db_from_huggingface(
        repo_id="CRM-LLM-01/CRM_Dataset",
        filename="instruction_finetune_data_new.db",
        token="hf_jcTCGfJxqjRJzxMYkySjEBlrYIQ"
    )

    # Step 2: Extract user queries from tables
    print("[üîç] Extracting queries from fine-tuning DB...")
    queries = extract_questions_from_all_tables(db_path)
    print(f"[üì•] Retrieved {len(queries)} queries")

    # Step 3: Format queries into prompts
    print("[üõ†Ô∏è] Generating prompts...")
    prompts = generate_instruction_prompts(queries)

    # Step 4: Save prompts to file
    print("[üíæ] Saving prompts to text file...")
    save_prompts_to_file(prompts)

    # Step 5: Load tokenizer + model and sample completions
    print("[üß†] Loading tokenizer and fine-tuned model...")
    tokenizer = Tokenizer(model_path="../Llama-3.2-1B/original/tokenizer.model")
    LLAMA32_CONFIG_1B["context_length"] = 1024 
    model = Llama3Model(LLAMA32_CONFIG_1B)
    checkpoint = torch.load(finetuned_GQA_model_path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])
    model.to(device)
    model.eval()


    print("[üéØ] Generating completions for 10,000 prompts...")
    sample_completions(
        model=model,
        tokenizer=tokenizer,
        prompt_file="rejection_sampling_prompts.txt",
        output_file="sampled_completions.json",
        max_prompts=100
    )
