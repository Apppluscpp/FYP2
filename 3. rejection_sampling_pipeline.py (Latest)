import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import sqlite3
import json
import torch
from tqdm import tqdm
from huggingface_hub import hf_hub_download
from llama_prompt import InstructionPrompt
from llama_structure import Llama3Model
from llama_training_speedup_utils import Tokenizer, generate_text_simple
from parameters import LLAMA32_CONFIG_1B
from config import finetuned_GQA_model_path 
from sentence_transformers import SentenceTransformer, util
import torch
import torch.nn.functional as F
import re  # for cleaning
from typing import List
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import heapq
import json

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# === Step 1: Download DB from Hugging Face ===

def fetch_db_from_huggingface(repo_id, filename, token):
    db_path = hf_hub_download(
        repo_id=repo_id,
        filename=filename,
        repo_type="dataset",
        token=token
    )
    print(f"[游닌] Downloaded database to: {db_path}")
    return db_path


# === Step 2: Extract Queries from All Tables ===

def extract_questions_from_all_tables(db_path):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    cursor.execute("SELECT name FROM sqlite_master WHERE type='table';")
    tables = [row[0] for row in cursor.fetchall()]

    questions = []
    for table in tables:
        try:
            cursor.execute(f"PRAGMA table_info({table});")
            columns = [row[1] for row in cursor.fetchall()]
            if 'question' in columns:
                cursor.execute(f"SELECT question FROM {table};")
                rows = cursor.fetchall()
                questions.extend([row[0] for row in rows])
            else:
                print(f"[丘멆잺] Table {table} has no 'question' column.")
        except Exception as e:
            print(f"[仇] Error in table {table}: {e}")

    conn.close()
    return questions


# === Step 3: Format Prompts ===

# def generate_instruction_prompts(queries, system_prompt="You are a helpful assistant"):
#     formatter = InstructionPrompt(system_prompt=system_prompt)
#     prompts = [f"### Instruction:\n{q}\n\n### Response:" for q in queries]
#     return prompts


# === Step 4: Save Prompts to File ===

def save_prompts_to_file(prompts, filename="rejection_sampling_prompts.txt"):
    with open(filename, "w", encoding="utf-8") as f:
        for prompt in prompts:
            f.write(prompt + "\n\n")
    print(f"[九] Saved {len(prompts)} prompts to {filename}")


# === Step 5: Sample Completions ===

def generate_text_sampled(model, idx, max_new_tokens, context_size, temperature=1.0, top_k=20):
    """
    Generate text using top-k sampling.
    """
    model.eval()
    for _ in range(max_new_tokens):
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)
        logits = logits[:, -1, :] / temperature

        if top_k > 0:
            top_k_values, _ = torch.topk(logits, top_k)
            cutoff = top_k_values[:, -1].unsqueeze(-1)
            logits[logits < cutoff] = -float('Inf')

        probs = F.softmax(logits, dim=-1)
        idx_next = torch.multinomial(probs, num_samples=1)
        idx = torch.cat((idx, idx_next), dim=1)

    return idx


# def truncate_at_special_token(text: str) -> str:
#     """
#     Truncate the generated text at the first appearance of special tokens or noisy content.
#     """
#     stop_tokens = [
#         "<|eot_id|>", "<|end_of_text|>", "<|begin_of_text|>",
#         "<|start_header_id|>", "user<|end_header_id|>", "system<|end_header_id|>"
#     ]
#     for token in stop_tokens:
#         if token in text:
#             return text.split(token)[0].strip()
#     return text.strip()


# === SAMPLING ===
def sample_completions(model, tokenizer, prompt_file, output_file, max_prompts=10000, completions_per_prompt=5, batch_size=32):
    with open(prompt_file, "r", encoding="utf-8") as f:
        all_prompts = f.read().strip().split("\n\n")

    prompts = all_prompts[:max_prompts]
    all_samples = []

    print(f"[游꿢] Sampling {completions_per_prompt} completions each for {len(prompts)} prompts (batch size = {batch_size})...")

    for i in tqdm(range(0, len(prompts), batch_size), desc="Sampling"):
        batch_prompts = prompts[i:i + batch_size]

        # Repeat each prompt for N completions
        expanded_prompts = []
        for prompt in batch_prompts:
            expanded_prompts.extend([prompt] * completions_per_prompt)

        # Tokenize all
        inputs = tokenizer(expanded_prompts, return_tensors="pt", padding=True, truncation=True).to(model.device)

        # Generate in batch
        outputs = model.generate(
            **inputs,
            do_sample=True,
            temperature=0.7,
            top_k=25,
            max_new_tokens=256,
            pad_token_id=tokenizer.eos_token_id
        )

        decoded = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)

        # Reconstruct per-prompt completions
        for j, prompt in enumerate(batch_prompts):
            completions = decoded[j * completions_per_prompt:(j + 1) * completions_per_prompt]
            all_samples.append({
                "prompt": prompt,
                "completions": [c.strip() for c in completions]
            })

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(all_samples, f, indent=2)

    print(f"[九] Saved completions to {output_file}")

def score_completions_with_biencoder_batched(input_file, output_file, model_name="sentence-transformers/all-MiniLM-L6-v2", batch_size=64):
    print(f"[游닌] Loading bi-encoder model: {model_name}")
    biencoder = SentenceTransformer(model_name)
    biencoder.eval()

    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    scored = []

    for item in tqdm(data, desc="Scoring with Bi-Encoder"):
        prompt = item["prompt"]
        completions = item["completions"]

        # Encode prompt once
        prompt_embedding = biencoder.encode(prompt, convert_to_tensor=True)

        # Encode all completions in batch
        completion_embeddings = biencoder.encode(completions, batch_size=batch_size, convert_to_tensor=True)

        # Compute cosine similarities
        scores = util.pytorch_cos_sim(prompt_embedding, completion_embeddings)[0]

        scored_completions = [
            {"text": c, "score": round(float(s), 4)}
            for c, s in zip(completions, scores)
        ]

        scored.append({
            "prompt": prompt,
            "scored_completions": scored_completions
        })

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(scored, f, indent=2)

    print(f"[九] Saved fast Bi-Encoder scores to: {output_file}")

def fast_filter_top_completions(input_file="scored_completions.json",
                                 output_file="filtered_completions.json",
                                 top_k=1,
                                 score_threshold=None):
    """
    Efficiently filters the top-k completions for each prompt using heapq.

    Args:
        input_file (str): Path to the scored completions JSON file.
        output_file (str): Where to save the filtered completions.
        top_k (int): Number of top completions to keep per prompt.
        score_threshold (float or None): Optional minimum score to include.
    """
    with open(input_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # Preallocate result list
    filtered = []

    for entry in data:
        prompt = entry["prompt"]
        completions = entry["scored_completions"]

        if score_threshold is not None:
            completions = [c for c in completions if c["score"] >= score_threshold]

        # Use heapq.nlargest for faster top-k selection
        top_comps = heapq.nlargest(top_k, completions, key=lambda x: x["score"])

        for comp in top_comps:
            filtered.append({
                "prompt": prompt,
                "completion": comp["text"].strip(),
                "score": comp["score"]
            })

    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(filtered, f, indent=2)

    print(f"[九] Fast filtered {len(filtered)} completions saved to {output_file}")

def convert_to_finetune_format(filtered_file="filtered_completions.json",
                               output_file="filtered_reinforced_data.txt"):
    """
    Converts filtered completions into <|user|>/<|assistant|> training format.
    """
    with open(filtered_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    formatted = []

    for item in data:
        user_prompt = item["prompt"].strip()
        assistant_reply = item["completion"].strip()

        formatted_text = (
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
            "You are a helpful assistant<|eot_id|>\n"
            "<|start_header_id|>user<|end_header_id|>\n"
            f"{user_prompt}<|eot_id|>\n"
            "<|start_header_id|>assistant<|end_header_id|>\n"
            f"{assistant_reply}<|eot_id|><|end_of_text|>\n"
        )

        formatted.append(formatted_text)

    with open(output_file, "w", encoding="utf-8") as f:
        f.write("\n".join(formatted))

    print(f"[游닍] Saved {len(formatted)} examples to {output_file} for fine-tuning")

# === MAIN ===

if __name__ == "__main__":
    print("[游대] Starting Rejection Sampling Prompt Generation Pipeline...")

    # Step 1: Download the instruction fine-tune DB
    db_path = fetch_db_from_huggingface(
        repo_id="CRM-LLM-01/CRM_Dataset",
        filename="instruction_finetune_data_new.db",
        token="hf_jcTCGfJxqjRJzxMYkySjEBlrYIQ"
    )

    # Step 2: Extract user queries from tables
    print("[游댌] Extracting queries from fine-tuning DB...")
    queries = extract_questions_from_all_tables(db_path)
    print(f"[游닌] Retrieved {len(queries)} queries")

    # Step 3: Format queries into prompts
    print("[游멆잺] Generating prompts...")
    prompts = queries

    # Step 4: Save prompts to file
    print("[游쬫 Saving prompts to text file...")
    save_prompts_to_file(prompts)

    # Step 5: Load tokenizer + model and sample completions
    print("[游] Loading tiiuae/falcon-7b-instruct model from HuggingFace...")
    model_name = "tiiuae/falcon-7b-instruct"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
    model.eval()

    print("[游꿢] Generating completions for 10000 prompts...")
    sample_completions(
        model=model,
        tokenizer=tokenizer,
        prompt_file="rejection_sampling_prompts.txt",
        output_file="sampled_completions.json",
        max_prompts=10000
    )

    print("[游늵] Scoring sampled completions using reward model...")
    score_completions_with_biencoder_batched(
        input_file="sampled_completions.json",
        output_file="scored_completions.json"
    )

    print("[游빛] Filtering top completions...")
    fast_filter_top_completions(
        input_file="scored_completions.json",
        output_file="filtered_completions.json",
        top_k=3,
        score_threshold=0.5  
    )

    print("[游댢] Converting filtered completions to fine-tuning format...")
    convert_to_finetune_format(
    filtered_file="filtered_completions.json",
    output_file="filtered_reinforced_data.txt"
    )

